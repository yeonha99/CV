{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","private_outputs":true,"provenance":[],"mount_file_id":"1K81mrt_YCFxbtsID76bHgvTZH-UcWnXx","authorship_tag":"ABX9TyMW3ybVuei60kaPMmqr06wP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"55r0ybFkRMfY"},"source":["import os\n","from PIL import Image\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torch import nn\n","from torchvision import transforms\n","\n","\n","class CustomImageDataset(Dataset):\n","    def read_data_set(self):\n","\n","        all_img_files = []\n","        all_labels = []\n","\n","        class_names = os.walk(self.data_set_path).__next__()[1]\n","\n","        for index, class_name in enumerate(class_names):\n","            label = index\n","            img_dir = os.path.join(self.data_set_path, class_name)\n","            img_files = os.walk(img_dir).__next__()[2]\n","\n","            for img_file in img_files:\n","                img_file = os.path.join(img_dir, img_file)\n","                img = Image.open(img_file)\n","                if img is not None:\n","                    all_img_files.append(img_file)\n","                    all_labels.append(label)\n","\n","        return all_img_files, all_labels, len(all_img_files), len(class_names)\n","\n","    def __init__(self, data_set_path, transforms=None):\n","        self.data_set_path = data_set_path\n","        self.image_files_path, self.labels, self.length, self.num_classes = self.read_data_set()\n","        self.transforms = transforms\n","\n","    def __getitem__(self, index):\n","        image = Image.open(self.image_files_path[index])\n","        image = image.convert(\"RGB\")\n","\n","        if self.transforms is not None:\n","            image = self.transforms(image)\n","\n","        return {'image': image, 'label': self.labels[index]}\n","\n","    def __len__(self):\n","        return self.length\n","\n","\n","class CustomConvNet(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CustomConvNet, self).__init__()\n","\n","        self.layer1 = self.conv_module(3, 16)\n","        self.layer2 = self.conv_module(16, 32)\n","        self.layer3 = self.conv_module(32, 64)\n","        self.layer4 = self.conv_module(64, 128)\n","        self.layer5 = self.conv_module(128, 256)\n","        self.gap = self.global_avg_pool(256, num_classes)\n","\n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = self.layer5(out)\n","        out = self.gap(out)\n","        out = out.view(-1, num_classes)\n","\n","        return out\n","\n","    def conv_module(self, in_num, out_num):\n","        return nn.Sequential(\n","            nn.Conv2d(in_num, out_num, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_num),\n","            nn.LeakyReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","\n","    def global_avg_pool(self, in_num, out_num):\n","        return nn.Sequential(\n","            nn.Conv2d(in_num, out_num, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(out_num),\n","            nn.LeakyReLU(),\n","            nn.AdaptiveAvgPool2d((1, 1)))\n","\n","\n","hyper_param_epoch = 10000\n","hyper_param_batch = 8\n","hyper_param_learning_rate = 0.001\n","\n","transforms_train = transforms.Compose([transforms.Resize((128, 128)),\n","                                       transforms.RandomRotation(10.),\n","                                       transforms.ToTensor()])\n","\n","transforms_test = transforms.Compose([transforms.Resize((128, 128)),\n","                                      transforms.ToTensor()])\n","\n","train_data_set = CustomImageDataset(data_set_path=\"/content/drive/MyDrive/dataset/CV/train\", transforms=transforms_train)\n","train_loader = DataLoader(train_data_set, batch_size=hyper_param_batch, shuffle=True)\n","\n","test_data_set = CustomImageDataset(data_set_path=\"/content/drive/MyDrive/dataset/CV/test\", transforms=transforms_test)\n","test_loader = DataLoader(test_data_set, batch_size=hyper_param_batch, shuffle=True)\n","\n","if not (train_data_set.num_classes == test_data_set.num_classes):\n","    print(\"error: Numbers of class in training set and test set are not equal\")\n","    exit()\n","\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","num_classes = train_data_set.num_classes\n","custom_model = CustomConvNet(num_classes=num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(custom_model.parameters(), lr=hyper_param_learning_rate)\n","\n","for e in range(hyper_param_epoch):\n","    for i_batch, item in enumerate(train_loader):\n","        images = item['image'].to(device)\n","        labels = item['label'].to(device)\n","\n","        # Forward pass\n","        outputs = custom_model(images)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i_batch + 1) % hyper_param_batch == 0:\n","            print('Epoch [{}/{}], Loss: {:.4f}'\n","                  .format(e + 1, hyper_param_epoch, loss.item()))\n","\n","# Test the model\n","custom_model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for item in test_loader:\n","        images = item['image'].to(device)\n","        labels = item['label'].to(device)\n","        outputs = custom_model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += len(labels)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Test Accuracy of the model on the {} test images: {} %'.format(total, 100 * correct / total))"],"execution_count":null,"outputs":[]}]}